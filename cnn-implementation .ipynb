{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convolutional Neural Network\n\nCNN (Convolutional Neural Network) is a type of neural network commonly used for image recognition and processing tasks. The basic building blocks of a CNN are convolutional layers, which consist of a set of learnable filters that are applied to the input image to extract features. These filters slide over the input image and perform element-wise multiplication and summation operations to produce a feature map.\n\nThe output of the convolutional layers is typically passed through one or more pooling layers, which downsample the feature maps to reduce their spatial dimensionality and make the network more computationally efficient. Finally, the output of the pooling layers is passed through one or more fully connected layers, which perform a classification or regression task.\n\nCNNs are particularly effective for image classification tasks because they can automatically learn spatial hierarchies of features from the raw image data, without the need for hand-engineered feature extraction. This makes them well-suited for a wide range of computer vision applications, including object detection, segmentation, and recognition.","metadata":{}},{"cell_type":"markdown","source":"# Import Library Function","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom os import listdir\nfrom os.path import isfile, join \n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:06.558434Z","iopub.execute_input":"2023-03-21T14:16:06.559062Z","iopub.status.idle":"2023-03-21T14:16:09.538707Z","shell.execute_reply.started":"2023-03-21T14:16:06.559026Z","shell.execute_reply":"2023-03-21T14:16:09.537556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ResNet50 model pre-trained on the ImageNet dataset.\n\nResNet50 is a deep convolutional neural network architecture with 50 layers, which has achieved state-of-the-art performance on many computer vision tasks, such as object recognition and detection. The ImageNet dataset contains millions of labeled images, and training a model on this dataset can result in a highly accurate model for many image-related tasks.\n\nThe weights='imagenet' argument specifies that the pre-trained weights should be loaded from the ImageNet dataset. By using these pre-trained weights, the model is already trained to recognize many different types of objects, making it useful for a wide range of applications.","metadata":{}},{"cell_type":"code","source":"resnetModel = ResNet50(weights='imagenet')  ","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:09.546249Z","iopub.execute_input":"2023-03-21T14:16:09.547217Z","iopub.status.idle":"2023-03-21T14:16:12.568231Z","shell.execute_reply.started":"2023-03-21T14:16:09.547176Z","shell.execute_reply":"2023-03-21T14:16:12.567089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagePath = \"../input/dogpic/Dog.jpg\"","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:12.569819Z","iopub.execute_input":"2023-03-21T14:16:12.570230Z","iopub.status.idle":"2023-03-21T14:16:12.574778Z","shell.execute_reply.started":"2023-03-21T14:16:12.570189Z","shell.execute_reply":"2023-03-21T14:16:12.573575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **image.load_img(imagePath, target_size=(224, 224))** loads an image from the specified file path and resizes it to a target size of 224x224 pixels using the Python Imaging Library (PIL).\n\n* **image.img_to_array(img)** converts the PIL image object to a NumPy array, which can be processed by the model.\n\n* **np.expand_dims(x, axis=0)** adds an extra dimension to the array at position 0. This is done to create a batch of images, as most deep learning models take inputs in batches. The resulting array now has a shape of (1, 224, 224, 3), where the first dimension is the batch size, the second and third dimensions are the image dimensions, and the fourth dimension represents the color channels (RGB).\n\n* **preprocess_input(x)** performs pre-processing on the input image array. In the case of ResNet50, this function applies a set of preprocessing operations, such as normalization and color channel reordering, that are specific to this model architecture. This helps to ensure that the input data is in a suitable format for the model to process and produce accurate results.","metadata":{}},{"cell_type":"code","source":"img = image.load_img(imagePath, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis = 0)\nx = preprocess_input(x)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:12.578601Z","iopub.execute_input":"2023-03-21T14:16:12.579302Z","iopub.status.idle":"2023-03-21T14:16:12.602449Z","shell.execute_reply.started":"2023-03-21T14:16:12.579263Z","shell.execute_reply":"2023-03-21T14:16:12.601535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:12.603957Z","iopub.execute_input":"2023-03-21T14:16:12.604322Z","iopub.status.idle":"2023-03-21T14:16:12.615736Z","shell.execute_reply.started":"2023-03-21T14:16:12.604286Z","shell.execute_reply":"2023-03-21T14:16:12.614226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelPredictions = resnetModel.predict(x)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:12.617185Z","iopub.execute_input":"2023-03-21T14:16:12.617949Z","iopub.status.idle":"2023-03-21T14:16:15.058065Z","shell.execute_reply.started":"2023-03-21T14:16:12.617910Z","shell.execute_reply":"2023-03-21T14:16:15.056880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imageRead = plt.imread(\"../input/dogpic/Dog.jpg\")\nplt.imshow(imageRead)\nplt.title(\"Original Image before the Predictions\")\nplt.show()\n\n# print(\"Predictions Result is : {}\".format(decode_predictions(modelPredictions, top = 3)[0]))\n# _, classnames, accuracy  = decode_predictions(modelPredictions, top = 3)[0][0]\n# print(\"Predicted Class : {}\\nAccuracy: {}%\".format(classnames,round( accuracy, 2)*100))\n\n_, classnames, accuracy  = decode_predictions(modelPredictions, top = 3)[0][0]\nr= classnames.split(\"_\")\nclassNamesFirst, classNamesSecond = r[0].title(), r[1].title()\nprint(\"Predictions Result is : {}\".format(decode_predictions(modelPredictions, top = 3)[0]))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.060123Z","iopub.execute_input":"2023-03-21T14:16:15.061144Z","iopub.status.idle":"2023-03-21T14:16:15.408433Z","shell.execute_reply.started":"2023-03-21T14:16:15.061099Z","shell.execute_reply":"2023-03-21T14:16:15.407362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a, b, c = [(3, 4, 5)]","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.410159Z","iopub.execute_input":"2023-03-21T14:16:15.410499Z","iopub.status.idle":"2023-03-21T14:16:15.415364Z","shell.execute_reply.started":"2023-03-21T14:16:15.410466Z","shell.execute_reply":"2023-03-21T14:16:15.414170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n 1. ancode_predictions\n 2. decode_predictions","metadata":{}},{"cell_type":"code","source":"decode_predictions(modelPredictions, top = 3)        # top=3 --> return the top three predictions.","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.417265Z","iopub.execute_input":"2023-03-21T14:16:15.417657Z","iopub.status.idle":"2023-03-21T14:16:15.427946Z","shell.execute_reply.started":"2023-03-21T14:16:15.417617Z","shell.execute_reply":"2023-03-21T14:16:15.426708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, classnames, accuracy  = decode_predictions(modelPredictions, top = 3)[0][0]\nprint(\"Predicted Class : {}\\nAccuracy: {}%\".format(classnames,round( accuracy, 2)*100))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.429538Z","iopub.execute_input":"2023-03-21T14:16:15.431027Z","iopub.status.idle":"2023-03-21T14:16:15.438234Z","shell.execute_reply.started":"2023-03-21T14:16:15.430988Z","shell.execute_reply":"2023-03-21T14:16:15.437036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, classnames, accuracy  = decode_predictions(modelPredictions, top = 3)[0][0]\nr= classnames.split(\"_\")\nclassNamesFirst, classNamesSecond = r[0].title(), r[1].title()\nprint(\"Predicted Class : {}\\nAccuracy: {}%\".format(classNamesFirst, classNamesSecond, accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.439891Z","iopub.execute_input":"2023-03-21T14:16:15.440863Z","iopub.status.idle":"2023-03-21T14:16:15.448561Z","shell.execute_reply.started":"2023-03-21T14:16:15.440806Z","shell.execute_reply":"2023-03-21T14:16:15.447308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r= classnames.split(\"_\")","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.450207Z","iopub.execute_input":"2023-03-21T14:16:15.451271Z","iopub.status.idle":"2023-03-21T14:16:15.456989Z","shell.execute_reply.started":"2023-03-21T14:16:15.451232Z","shell.execute_reply":"2023-03-21T14:16:15.455797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classNamesFirst, classNamesSecond = r[0].title(), r[1].title()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.461694Z","iopub.execute_input":"2023-03-21T14:16:15.462475Z","iopub.status.idle":"2023-03-21T14:16:15.467626Z","shell.execute_reply.started":"2023-03-21T14:16:15.462429Z","shell.execute_reply":"2023-03-21T14:16:15.466503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # Design","metadata":{}},{"cell_type":"code","source":"# model = Sequential()\n\n# # Input Layer 01\n# model.add(Conv2D())\n# model.add(MaxPooling2D(pool_size = (2, 2)))\n\n\n# # Hidden Layer 01\n# model.add(Conv2D())\n# model.add(MaxPooling2D(pool_size = (2, 2)))\n\n\n# # Hidden Layer 02\n# model.add(Conv2D())\n# model.add(MaxPooling2D(pool_size = (2, 2)))\n\n\n# # Hidden Layer 03\n# model.add(Conv2D(128 ))\n# model.add(MaxPooling2D(pool_size = (2, 2)))\n\n# # Flatten Layer \n# model.add(Flatten())\n\n# # Fully Connected Neural Network\n# model.add(Dense(128))\n# model.add(Densen(number_of_classes))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.469108Z","iopub.execute_input":"2023-03-21T14:16:15.469785Z","iopub.status.idle":"2023-03-21T14:16:15.477484Z","shell.execute_reply.started":"2023-03-21T14:16:15.469747Z","shell.execute_reply":"2023-03-21T14:16:15.476605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cv2.**copyMakeBorder** function is used to create a new image with a border around the input image. It takes several arguments:\n\n**input_image:** the input image that will have a border added to it\n\n**0:** the number of pixels to add to the top of the image \n    \n**0:** the number of pixels to add to the bottom of the image \n    \n**0:** the number of pixels to add to the left of the image \n\n\n**imageL.shape[1]+300:** the number of pixels to add to the right of the image. imageL is a variable that is not defined in the code snippet you provided, so it's not clear what its shape is. The +300 is used to add an additional 300 pixels to the border.\n\n**cv2.BORDER_CONSTANT:** the type of border to add. In this case, a constant value is used to fill the border pixels.\n\n**value = backgroundColor:** the color of the border pixels. backgroundColor is a list of three values representing the blue, green, and red color channels.\n\nThe resulting expandImage is the original input image with a border added to the right side. The border has a width of imageL.shape[1]+300 pixels and is filled with the backgroundColor.","metadata":{}},{"cell_type":"markdown","source":"# PutText Function\n\n\n\n**expandImage** is the image to which the text will be added.\n\n**str(names)** is the text to be added. names is likely a variable containing a list of names, and str() is being used to convert that list into a string.\n\n**(image_width + 50, 50)** is the coordinates where the text will be added. \n\n**(image_width + 50, 50)** likely means that the text will be added to the right of the original image, 50 pixels to the right of the rightmost edge, and 50 pixels down from the top edge.\n\n**cv2.FONT_HERSHEY_COMPLEX_SMALL** is the font used to render the text. This is a built-in font provided by OpenCV.\n\n**2** is the font scale. This determines the size of the text.\n\n**(255, 255, 255)** is the color of the text. In this case, it is white, which is represented by the RGB color (255, 255, 255).\n\n**1** is the thickness of the text. This determines how thick the lines of the text will be.","metadata":{}},{"cell_type":"code","source":"\n\ndef drawTextonImage(names, predictions, input_image):\n    backgroundColor = [0, 0, 0]\n    expandImage = cv2.copyMakeBorder(input_image, 0, 0, 0, imageL.shape[1]+300, cv2.BORDER_CONSTANT, value = backgroundColor)\n    image_width = input_image.shape[1]\n    \n    for (i, predictions) in enumerate(predictions):\n        imageStrings = str(predictions[1])  + \" \" + str(predictions[2])\n        \n        cv2.putText(expandImage, str(names), (image_width + 50, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (255, 255, 255), 1)\n        cv2.putText(expandImage, imageStrings, (image_width + 50, 50+((i+1)*50)), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), 1)\n        plt.imshow(expandImage)\n    \nimagePath = \"../input/cat-and-dogs/dataset/test_set/dogs/\"\nimageFileName = [f for f in listdir(imagePath) if isfile(join(imagePath, f))]","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:15.478691Z","iopub.execute_input":"2023-03-21T14:16:15.479267Z","iopub.status.idle":"2023-03-21T14:16:16.160990Z","shell.execute_reply.started":"2023-03-21T14:16:15.479226Z","shell.execute_reply":"2023-03-21T14:16:16.159894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Resize\n\n**cv2.resize():** This is a function from the OpenCV library that is used to resize images.\n\n**imageNew:** This is the image that is being resized.\n\n**None:** This parameter specifies the output image size. In this case, None means that the output size will be determined automatically based on the scaling factors.\n\n**fx = 0.5, fy = 0.5:** These are the scaling factors for the x and y dimensions, respectively. A scaling factor of 0.5 means that the image will be resized to half of its original size in that dimension.\n\n**interpolation = cv2.INTER_CUBIC:** This parameter specifies the interpolation method to use. In this case, cubic interpolation is being used, which is a method that uses a cubic spline to interpolate pixel values. It tends to produce smoother results than other interpolation methods, but can be slower.\n\n\n\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nfor file in imageFileName:\n    img = image.load_img(imagePath+file, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis = 0)\n    x = preprocess_input(x)\n    \n    imageNew = cv2.imread(imagePath+file)\n    imageL = cv2.resize(imageNew, None, fx = 0.5, fy = 0.5, interpolation = cv2.INTER_CUBIC)\n    \n    predictions = resnetModel.predict(x)\n    predictions = decode_predictions(predictions, top = 3)[0]\n    drawTextonImage(\"Image Predictions \", predictions, imageL)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:16:16.162443Z","iopub.execute_input":"2023-03-21T14:16:16.163101Z","iopub.status.idle":"2023-03-21T14:19:12.081563Z","shell.execute_reply.started":"2023-03-21T14:16:16.163060Z","shell.execute_reply":"2023-03-21T14:19:12.079780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VGG16\n\nVGG16 is a convolutional neural network (CNN) architecture. \n\nThe VGG16 network consists of 16 layers, including 13 convolutional layers and 3 fully connected layers. It has a fixed input size of 224 x 224 x 3 and uses a set of small convolutional filters (3x3) throughout the network with a fixed stride of 1 pixel and same padding. The network also uses max-pooling layers (2x2) with a stride of 2 pixels to reduce the spatial size of the feature maps.\n\nThe architecture of VGG16 can be summarized as follows:\n\nInput layer (224 x 224 x 3)\n\nConvolutional layer with 64 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 64 filters, each of size 3x3, with ReLU activation\n\nMax-pooling layer with a pool size of 2x2 and stride of 2 pixels\n\nConvolutional layer with 128 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 128 filters, each of size 3x3, with ReLU activation\n\nMax-pooling layer with a pool size of 2x2 and stride of 2 pixels\n\nConvolutional layer with 256 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 256 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 256 filters, each of size 3x3, with ReLU activation\n\nMax-pooling layer with a pool size of 2x2 and stride of 2 pixels\n\nConvolutional layer with 512 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 512 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 512 filters, each of size 3x3, with ReLU activation\n\nMax-pooling layer with a pool size of 2x2 and stride of 2 pixels\n\nConvolutional layer with 512 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 512 filters, each of size 3x3, with ReLU activation\n\nConvolutional layer with 512 filters, each of size 3x3, with ReLU activation\n\nMax-pooling layer with a pool size of 2x2 and stride of 2 pixels\n\nFully connected layer with 4096 units and ReLU activation\n\nDropout layer with a rate of 0.5\n\nFully connected layer with 4096 units and ReLU activation\n\nDropout layer with a rate of 0.5\n\nFully connected layer with 1000 units and softmax activation (output layer)\n\nThe weights of the VGG16 network were pre-trained on the ImageNet dataset, which contains millions of labeled images. This pre-training allows the network to learn a set of general features that can be useful for a wide range of computer vision tasks. The pre-trained weights can then be fine-tuned on a smaller dataset for a specific task ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import vgg16, inception_v3, resnet50\nvgg_model = vgg16.VGG16(weights='imagenet')\ninception_model = inception_v3.InceptionV3(weights='imagenet')\nresnet_model = resnet50.ResNet50(weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:19:12.083098Z","iopub.execute_input":"2023-03-21T14:19:12.084144Z","iopub.status.idle":"2023-03-21T14:19:18.310882Z","shell.execute_reply.started":"2023-03-21T14:19:12.084102Z","shell.execute_reply":"2023-03-21T14:19:18.309759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imagePath = \"../input/cat-and-dogs/dataset/test_set/dogs/\"\nimageFileName = [f for f in listdir(imagePath) if isfile(join(imagePath, f))]\n\nfor file in imageFileName:\n\n    from tensorflow.keras.preprocessing import image # Need to reload as opencv2 seems to have a conflict\n    img = image.load_img(imagePath+file, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    \n    #load image using opencv\n    img2 = cv2.imread(imagePath+file)\n    imageL = cv2.resize(img2, None, fx=.5, fy=.5, interpolation = cv2.INTER_CUBIC) \n    \n    # Get VGG16 Predictions\n    preds_vgg_model = vgg_model.predict(x)\n    preditions_vgg = decode_predictions(preds_vgg_model, top=3)[0]\n    drawTextonImage(\"VGG16 Predictions\", preditions_vgg, imageL) \n\n    \n    # Get ResNet50 Predictions\n    preds_resnet = resnet_model.predict(x)\n    preditions_resnet = decode_predictions(preds_resnet, top=3)[0]\n    drawTextonImage(\"ResNet50 Predictions\", preditions_resnet, imageL) \n    cv2.waitKey(0)\n\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T14:19:18.312544Z","iopub.execute_input":"2023-03-21T14:19:18.312934Z","iopub.status.idle":"2023-03-21T14:19:21.442333Z","shell.execute_reply.started":"2023-03-21T14:19:18.312893Z","shell.execute_reply":"2023-03-21T14:19:21.440796Z"},"trusted":true},"execution_count":null,"outputs":[]}]}